{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM ê¸°ë³¸ íŠœí† ë¦¬ì–¼\n",
    "\n",
    "### ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë°°ìš¸ ë‚´ìš©\n",
    "\n",
    "- vLLM ì„¤ì¹˜ ë° ì„œë²„ ì‹œì‘\n",
    "- OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ vLLM ì‚¬ìš©í•˜ê¸°\n",
    "- ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„± ë° ì±„íŒ…\n",
    "- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì‹¤í–‰\n",
    "- ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "vLLMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” GPUê°€ ìˆëŠ” í™˜ê²½ì´ ê¶Œì¥ëœë‹¤. CPUì—ì„œë„ ì‘ë™í•˜ì§€ë§Œ ì„±ëŠ¥ì´ ì œí•œì ì´ë‹¤.\n",
    "\n",
    "### ì„¤ì¹˜\n",
    "\n",
    "```bash\n",
    "pip install vllm\n",
    "```\n",
    "\n",
    "### ì„œë²„ ì‹œì‘\n",
    "\n",
    "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ vLLM ì„œë²„ë¥¼ ì‹œì‘í•œë‹¤:\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server --model openai/gpt-oss-20b --port 8001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "#%pip install openai vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/lecture/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-17 23:06:02 [__init__.py:216] Automatically detected platform cuda.\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "vllm_path = \"llms\"\n",
    "sys.path.append(os.path.dirname(vllm_path))\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device_type = \"mps\" if use_mps else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì—ˆë‹¤\n",
      "ì„œë²„ ì£¼ì†Œ: base_url\n",
      "ëª¨ë¸: openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "# vLLM ì„œë²„ì— ì—°ê²°í•˜ëŠ” OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8001/v1\",  # vLLM ì„œë²„ ì£¼ì†Œ\n",
    "    api_key=\"EMPTY\"  # vLLMì€ API í‚¤ê°€ í•„ìš”ì—†ì§€ë§Œ í´ë¼ì´ì–¸íŠ¸ëŠ” ê°’ì„ ìš”êµ¬í•œë‹¤\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„\n",
    "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "\n",
    "print(\"vLLM í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì—ˆë‹¤\")\n",
    "print(f\"ì„œë²„ ì£¼ì†Œ: base_url\")\n",
    "print(f\"ëª¨ë¸: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í”„ë¡¬í”„íŠ¸: Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ”\n",
      "ìƒì„± ê²°ê³¼:  ì£¼ë¡œ ê³¼í•™, ê³µí•™, ë°ì´í„° ê³¼í•™ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ê³¼í•™ ë¶„ì•¼ì—ì„œ ì—¬ëŸ¬ ë¶„ì•¼ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë„êµ¬ë“¤ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Pythonì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "Pythonì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ëŠ” ì´ìœ ëŠ” ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë“¤ ë•Œë¬¸ì…ë‹ˆë‹¤:\n",
      "\n",
      "1. **ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬**: Pythonì€ ê³¼í•™, ê³µí•™,\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "def generate_completion(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤\n",
    "    \n",
    "    Args:\n",
    "        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸\n",
    "        max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    response = client.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "prompt = \"Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ”\"\n",
    "result = generate_completion(prompt)\n",
    "\n",
    "print(f\"í”„ë¡¬í”„íŠ¸: {prompt}\")\n",
    "print(f\"ìƒì„± ê²°ê³¼: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì±„íŒ… API ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì: ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: ### ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì´ë€?\n",
      "\n",
      "**ë¨¸ì‹ ëŸ¬ë‹**ì€ ì»´í“¨í„°ê°€ ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ íŒ¨í„´ì„ ì°¾ì•„ë‚´ê³ , ê·¸ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê±°ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê³¼í•™ê³¼ ê¸°ìˆ ì…ë‹ˆë‹¤.  \n",
      "ë‹¤ì‹œ ë§í•´, â€œì‚¬ëŒì´ ì§ì ‘ í”„ë¡œê·¸ë˜ë°í•˜ì§€ ì•Šì•„ë„, ê²½í—˜(ë°ì´í„°)ì„ í†µí•´ í•™ìŠµí•˜ê³  ê°œì„ ë˜ëŠ” ì»´í“¨í„° í”„ë¡œê·¸ë¨â€ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. ë¨¸ì‹ ëŸ¬ë‹ì´ í•„ìš”í•œ ì´ìœ \n",
      "\n",
      "| ë¬¸ì œ | ì „í†µì  í”„ë¡œê·¸ë˜ë° ë°©ì‹ | ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ |\n",
      "|------|---------------------|--------------|\n",
      "| **ë³µì¡í•œ íŒ¨í„´ ì¸ì‹** (ìŒì„±, ì´ë¯¸ì§€, ìì—°ì–´ ë“±) | ìˆ˜ì‘ì—…ìœ¼ë¡œ ê·œì¹™ì„ ì •ì˜ â†’ ì–´ë ¤ì›€ | ë°ì´í„°ë¥¼ í†µí•´ íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ |\n",
      "| **ë°ì´í„° ì–‘ì´ ë§ê³  ë¹ ë¥´ê²Œ ë³€í•¨** | ì‚¬ëŒ ìˆ˜ë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ â†’ ë¹„íš¨ìœ¨ | ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í•™ìŠµ ì£¼ê¸°ë¥¼ ì¡°ì • |\n",
      "| **ë¹„ì„ í˜• ê´€ê³„** (ì˜ˆ: ì£¼ê°€, ì˜ë£Œ ì§„ë‹¨) | ì„ í˜•/ë¹„ì„ í˜• ìˆ˜ì‹ìœ¼ë¡œ ì–´ë ¤ì›€ | ë¹„ì„ í˜• ëª¨ë¸(ë”¥ëŸ¬ë‹ ë“±) í™œìš© |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. ë¨¸ì‹ ëŸ¬ë‹ì˜ í•µì‹¬ ê°œë…\n",
      "\n",
      "| ê°œë… | ì„¤ëª… |\n",
      "|------|------|\n",
      "| **ë°ì´í„°** | ëª¨ë¸ì´ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ì›ì²œ. ì…ë ¥(íŠ¹ì§•) + ë¼ë²¨(ì •ë‹µ) |\n",
      "| **ëª¨ë¸(ì•Œê³ ë¦¬ì¦˜)** | ì…ë ¥ â†’ ì¶œë ¥ì„ ì—°ê²°í•˜ëŠ” ìˆ˜í•™ì  êµ¬ì¡°. ì˜ˆ: ì„ í˜• íšŒê·€, ê²°ì • íŠ¸ë¦¬, ì‹ ê²½ë§ |\n",
      "| **í•™ìŠµ(Training)** | ë°ì´í„°ì™€ ëª¨ë¸ì„ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜(ë§¤ê°œë³€ìˆ˜)ë¥¼ ìµœì í™”. ì˜¤ì°¨ë¥¼ ì¤„ì´ë ¤ í•¨. |\n",
      "| **í‰ê°€(Evaluation)** | í•™ìŠµë˜ì§€ ì•Šì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ì„±\n"
     ]
    }
   ],
   "source": [
    "def chat(messages, temperature=0.7, max_tokens=500):\n",
    "    \"\"\"\n",
    "    ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ì±„íŒ…ì„ ì§„í–‰í•œë‹¤\n",
    "    \n",
    "    Args:\n",
    "        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "        temperature: ì°½ì˜ì„± ì¡°ì ˆ íŒŒë¼ë¯¸í„°\n",
    "        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ë‹¨ì¼ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\"}\n",
    "]\n",
    "\n",
    "response = chat(messages)\n",
    "print(f\"ì‚¬ìš©ì: {messages[0]['content']}\")\n",
    "print(f\"\\nì–´ì‹œìŠ¤í„´íŠ¸: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš© ì˜ˆì œ ===\n",
      "ì‹œìŠ¤í…œ ì—­í• : ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡œê·¸ë˜ë° ê°•ì‚¬ì…ë‹ˆë‹¤. ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì‹¤ìš©ì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì§ˆë¬¸: for ë£¨í”„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "ë‹µë³€: ## â€˜for ë£¨í”„â€™ë€?\n",
      "\n",
      "í”„ë¡œê·¸ë˜ë°ì—ì„œ **ë£¨í”„**(ë°˜ë³µë¬¸)ëŠ” â€œê°™ì€ ì¼ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?â€ ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•´ ì£¼ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.  \n",
      "â€˜for ë£¨í”„â€™ëŠ” **â€œì •í•´ì§„ ê°œìˆ˜ë§Œí¼ ë°˜ë³µí•´ë¼â€** ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°–ê³  ìˆì–´ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ë°˜ë³µë¬¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.  \n",
      "\n",
      "> **í•µì‹¬**  \n",
      "> * **ì‹œì‘ê°’** â†’ ë°˜ë³µì„ ì‹œì‘í•  ë•Œì˜ ê°’  \n",
      "> * **ì¢…ë£Œì¡°ê±´** â†’ ë°˜ë³µì„ ë©ˆì¶œ ì¡°ê±´(ë³´í†µ â€œì´ ê°’ì´ ì•„ë‹ˆë¼ë©´ ê³„ì†â€)  \n",
      "> * **ì¦ê°ì‹** â†’ í•œ ë²ˆ ë°˜ë³µì´ ëë‚œ ë’¤ ê°’ì´ ì–´ë–»ê²Œ ë°”ë€”ì§€ ì •ì˜  \n",
      "\n",
      "ì´ ì„¸ ê°€ì§€ë¥¼ ëª…í™•íˆ í•˜ë©´ â€œë°˜ë³µì´ ì–¸ì œ ì¢…ë£Œë˜ëŠ”ì§€ ì •í™•íˆ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬´í•œë£¨í”„ì— ë¹ ì§€ì§€ ì•ŠëŠ”ë‹¤â€ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Python ì˜ˆì‹œ\n",
      "\n",
      "Pythonì—ì„œëŠ” `for` ë£¨í”„ê°€ ê°€ì¥ ì§ê´€ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
      "ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœëŠ” **ì‹œí€€ìŠ¤(ë¦¬ìŠ¤íŠ¸, ë¬¸ìì—´, íŠœí”Œ ë“±)** ë¥¼ í•œ ë²ˆì”© ìˆœíšŒí•©ë‹ˆë‹¤.\n",
      "\n",
      "```python\n",
      "# 1ë²ˆë¶€í„° 5ë²ˆê¹Œì§€ ì¶œë ¥\n",
      "for i in range(1, 6):   # range(ì‹œì‘, ë+1)\n",
      "    print(i)\n",
      "```\n",
      "\n",
      "`range(1, 6)` ì€ 1, 2, 3, 4, 5 ë¥¼ ìˆœì„œëŒ€ë¡œ ìƒì„±í•˜ëŠ” ì´í„°ë ˆì´í„°ì…ë‹ˆë‹¤.  \n",
      "ìœ„ ì½”ë“œì˜ ì‹¤í–‰ ê²°ê³¼ëŠ”\n",
      "\n",
      "```\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "```\n",
      "\n",
      "### ë¦¬ìŠ¤íŠ¸ ìˆœíšŒí•˜ê¸°\n",
      "\n",
      "```python\n",
      "fruits = ['ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜', 'ì²´ë¦¬']\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "### ë¬¸ìì—´ ìˆœíšŒí•˜ê¸°\n",
      "\n",
      "```python\n",
      "for ch in \"hello\n"
     ]
    }
   ],
   "source": [
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ í¬í•¨í•œ ì±„íŒ…\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡œê·¸ë˜ë° ê°•ì‚¬ì…ë‹ˆë‹¤. ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì‹¤ìš©ì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"for ë£¨í”„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = chat(messages)\n",
    "print(\"=== ì‹œìŠ¤í…œ ë©”ì‹œì§€ í™œìš© ì˜ˆì œ ===\")\n",
    "print(f\"ì‹œìŠ¤í…œ ì—­í• : {messages[0]['content']}\")\n",
    "print(f\"\\nì§ˆë¬¸: {messages[1]['content']}\")\n",
    "print(f\"\\në‹µë³€: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì—°ì† ëŒ€í™” ì˜ˆì œ ===\n",
      "\n",
      "ì‚¬ìš©ì: ì €ëŠ” ë°ì´í„° ê³¼í•™ì„ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: ë°ì´í„° ê³¼í•™ì„ ê³µë¶€í•˜ê³  ê³„ì‹œë‹¤ë‹ˆ ë©‹ì§€ë„¤ìš”!  \n",
      "ì–´ë–¤ ì£¼ì œ(ì˜ˆ: ê¸°ê³„ í•™ìŠµ, í†µê³„, ì‹œê°í™”, ë°ì´í„° ì „ì²˜ë¦¬ ë“±)ë‚˜ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ê¹Œìš”? í˜¹ì€ ì§€ê¸ˆ ê²ªê³  ìˆëŠ” ì–´ë ¤ì›€ì´ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ˜Š\n",
      "\n",
      "ì‚¬ìš©ì: ì œê°€ ë¬´ì—‡ì„ ê³µë¶€í•œë‹¤ê³  í–ˆì£ ?\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: ë„¤, ì•ì„œ ë§ì”€í•˜ì‹  ë‚´ìš©ì€ â€œì €ëŠ” ë°ì´í„° ê³¼í•™ì„ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤.â€ì…ë‹ˆë‹¤.  \n",
      "ë°ì´í„° ê³¼í•™ì— ëŒ€í•´ ë” ê¹Šì´ ì•Œì•„ë³´ê³  ì‹¶ìœ¼ì‹  íŠ¹ì • ì˜ì—­(ì˜ˆ: ë¨¸ì‹ ëŸ¬ë‹, í†µê³„, ì‹œê°í™”, ë°ì´í„° ì „ì²˜ë¦¬ ë“±)ì´ ìˆìœ¼ì‹ ê°€ìš”? í•„ìš”í•˜ì‹  ìë£Œë‚˜ ì¡°ì–¸ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!\n",
      "\n",
      "í˜„ì¬ ëŒ€í™” ê¸¸ì´: 5ê°œ ë©”ì‹œì§€\n"
     ]
    }
   ],
   "source": [
    "# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def add_message_and_respond(user_message):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì‘ë‹µì„ ë°›ëŠ”ë‹¤\n",
    "    \n",
    "    Args:\n",
    "        user_message: ì‚¬ìš©ì ë©”ì‹œì§€\n",
    "    \n",
    "    Returns:\n",
    "        ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\n",
    "    \"\"\"\n",
    "    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
    "    conversation.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    })\n",
    "    \n",
    "    # ì‘ë‹µ ìƒì„±\n",
    "    assistant_response = chat(conversation)\n",
    "    \n",
    "    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€\n",
    "    conversation.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_response\n",
    "    })\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "# ì—°ì† ëŒ€í™” ì˜ˆì œ\n",
    "print(\"=== ì—°ì† ëŒ€í™” ì˜ˆì œ ===\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ëŒ€í™”\n",
    "user_msg_1 = \"ì €ëŠ” ë°ì´í„° ê³¼í•™ì„ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "response_1 = add_message_and_respond(user_msg_1)\n",
    "print(f\"\\nì‚¬ìš©ì: {user_msg_1}\")\n",
    "print(f\"ì–´ì‹œìŠ¤í„´íŠ¸: {response_1}\")\n",
    "\n",
    "# ë‘ ë²ˆì§¸ ëŒ€í™”\n",
    "user_msg_2 = \"ì œê°€ ë¬´ì—‡ì„ ê³µë¶€í•œë‹¤ê³  í–ˆì£ ?\"\n",
    "response_2 = add_message_and_respond(user_msg_2)\n",
    "print(f\"\\nì‚¬ìš©ì: {user_msg_2}\")\n",
    "print(f\"ì–´ì‹œìŠ¤í„´íŠ¸: {response_2}\")\n",
    "\n",
    "print(f\"\\ní˜„ì¬ ëŒ€í™” ê¸¸ì´: {len(conversation)}ê°œ ë©”ì‹œì§€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ ===\n",
      "\n",
      "**ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ê°œë… 3ê°€ì§€**\n",
      "\n",
      "| # | ê°œë… | í•µì‹¬ ë‚´ìš© | ì˜ˆì‹œ |\n",
      "|---|------|-----------|------|\n",
      "| 1 | **ë”¥ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬(Deep Neural Network)** | ì…ë ¥ â†’ ì—¬ëŸ¬ ê°œì˜ ì€ë‹‰ì¸µ(hidden layer) â†’ ì¶œë ¥ìœ¼ë¡œ êµ¬ì„±ëœ ì¸ê³µì‹ ê²½ë§. ê° ì¸µì€ **ê°€ì¤‘ì¹˜**ì™€ **í¸í–¥**ì„ í†µí•´ ì…ë ¥ì„ ë³€í™˜í•˜ê³ , **í™œì„±í™” í•¨ìˆ˜(activation function)** ë¡œ ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬í•´ ë³µì¡í•œ í•¨ìˆ˜ ê·¼ì‚¬ ê°€ëŠ¥. ì¸µì´ ë§ì„ìˆ˜ë¡ â€œê¹ŠìŒ(depth)â€ì´ ì¦ê°€í•´ ë” ê³ ì°¨ì›ì , ì¶”ìƒì ì¸ íŠ¹ì§•ì„ í•™ìŠµ. | ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œ ConvNet(í•©ì„±ê³± ì‹ ê²½ë§) â†’ Convâ€“ReLUâ€“Poolâ€“FC ë“± 10ì—¬ ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ëª¨ë¸. |\n",
      "| 2 | **ì—­ì „íŒŒ(Backpropagation) & ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)** | ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì¶œë ¥ê³¼ ì‹¤ì œ ì •ë‹µ(ë ˆì´ë¸”)\n"
     ]
    }
   ],
   "source": [
    "def chat_stream(messages, temperature=0.7, max_tokens=500):\n",
    "    \"\"\"\n",
    "    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì±„íŒ… ì‘ë‹µì„ ë°›ëŠ”ë‹¤\n",
    "    \n",
    "    Args:\n",
    "        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "        temperature: ì°½ì˜ì„± ì¡°ì ˆ íŒŒë¼ë¯¸í„°\n",
    "        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        ì™„ì „í•œ ì‘ë‹µ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”\n",
    "    stream = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”\n",
    "    )\n",
    "    \n",
    "    # ì „ì²´ ì‘ë‹µ ìˆ˜ì§‘\n",
    "    full_response = \"\"\n",
    "    \n",
    "    # ìŠ¤íŠ¸ë¦¼ì—ì„œ ì²­í¬ë¥¼ ë°›ì•„ ì²˜ë¦¬\n",
    "    for chunk in stream:\n",
    "        # ë¸íƒ€(ì¦ë¶„) ë‚´ìš© ì¶”ì¶œ\n",
    "        delta_content = chunk.choices[0].delta.content\n",
    "        \n",
    "        if delta_content:\n",
    "            full_response += delta_content\n",
    "            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥\n",
    "            print(delta_content, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # ì¤„ë°”ê¿ˆ\n",
    "    return full_response\n",
    "\n",
    "# ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸\n",
    "print(\"=== ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ ===\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ë”¥ëŸ¬ë‹ì˜ ì£¼ìš” ê°œë… 3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n",
    "]\n",
    "\n",
    "print(\"\\nì–´ì‹œìŠ¤í„´íŠ¸: \", end=\"\")\n",
    "response = chat_stream(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ ===\n",
      "\n",
      "5ê°œì˜ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬í•œë‹¤...\n",
      "\n",
      "[ìš”ì²­ 1]\n",
      "ì§ˆë¬¸: Javaì˜ ì£¼ìš” ìš©ë„ëŠ”?\n",
      "ë‹µë³€: ì‘ë‹µ ì—†ìŒ...\n",
      "\n",
      "[ìš”ì²­ 2]\n",
      "ì§ˆë¬¸: JavaScriptì˜ íŠ¹ì§•ì€?\n",
      "ë‹µë³€: ì‘ë‹µ ì—†ìŒ...\n",
      "\n",
      "[ìš”ì²­ 3]\n",
      "ì§ˆë¬¸: Pythonì˜ ì¥ì ì€?\n",
      "ë‹µë³€: ì‘ë‹µ ì—†ìŒ...\n",
      "\n",
      "[ìš”ì²­ 4]\n",
      "ì§ˆë¬¸: C++ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼ëŠ”?\n",
      "ë‹µë³€: ì‘ë‹µ ì—†ìŒ...\n",
      "\n",
      "[ìš”ì²­ 5]\n",
      "ì§ˆë¬¸: Go ì–¸ì–´ì˜ íŠ¹ì§•ì€?\n",
      "ë‹µë³€: ì‘ë‹µ ì—†ìŒ...\n",
      "\n",
      "ì´ ì²˜ë¦¬ ì‹œê°„: 0.66ì´ˆ\n",
      "í‰ê·  ì²˜ë¦¬ ì‹œê°„: 0.13ì´ˆ/ìš”ì²­\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def process_single_request(prompt):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ìš”ì²­ì„ ì²˜ë¦¬í•œë‹¤\n",
    "    \n",
    "    Args:\n",
    "        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = chat(messages, max_tokens=100)\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "# ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n",
    "prompts = [\n",
    "    \"Pythonì˜ ì¥ì ì€?\",\n",
    "    \"JavaScriptì˜ íŠ¹ì§•ì€?\",\n",
    "    \"Javaì˜ ì£¼ìš” ìš©ë„ëŠ”?\",\n",
    "    \"C++ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼ëŠ”?\",\n",
    "    \"Go ì–¸ì–´ì˜ íŠ¹ì§•ì€?\"\n",
    "]\n",
    "\n",
    "print(\"=== ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ ===\")\n",
    "print(f\"\\n{len(prompts)}ê°œì˜ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬í•œë‹¤...\\n\")\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "start_time = time.time()\n",
    "\n",
    "# ë³‘ë ¬ë¡œ ìš”ì²­ ì²˜ë¦¬\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì œì¶œ\n",
    "    future_to_prompt = {executor.submit(process_single_request, prompt): prompt for prompt in prompts}\n",
    "    \n",
    "    # ì™„ë£Œëœ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘\n",
    "    results = []\n",
    "    for future in concurrent.futures.as_completed(future_to_prompt):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "\n",
    "# ì¢…ë£Œ ì‹œê°„ ê¸°ë¡\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"[ìš”ì²­ {i}]\")\n",
    "    print(f\"ì§ˆë¬¸: {result['prompt']}\")\n",
    "    \n",
    "    # ì‘ë‹µì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if result.get('error', False):\n",
    "        print(f\"ë‹µë³€: {result['response']}\")\n",
    "    else:\n",
    "        # ì‘ë‹µ ê¸¸ì´ í™•ì¸\n",
    "        response_text = result['response'] if result['response'] else \"ì‘ë‹µ ì—†ìŒ\"\n",
    "        preview = response_text[:100] if len(response_text) > 100 else response_text\n",
    "        print(f\"ë‹µë³€: {preview}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"ì´ ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ\")\n",
    "print(f\"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed_time/len(prompts):.2f}ì´ˆ/ìš”ì²­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== íŒŒë¼ë¯¸í„° ë¹„êµ ì˜ˆì œ ===\n",
      "\n",
      "[Temperature = 0.2 (ê²°ì •ì )]\n",
      "**ì œëª©: ë§ˆì§€ë§‰ ìì‚¬ê·€**\n",
      "\n",
      "ë´„ì´ ëë‚˜ê°ˆ ë¬´ë µ, ì‘ì€ ë§ˆì„ì— ì‚¬ëŠ” ì†Œë…€ ë¯¼ì§€ëŠ” ë§¤ì¼ ì•„ì¹¨ ì°½ë¬¸ì„ ì—´ì–´ ë“¤ì´ë§ˆì‹œëŠ” ë°”ëŒì„ ì¢‹ì•„í–ˆë‹¤. ì–´ëŠ ë‚ , ê·¸ë…€ëŠ” ë§ˆë‹¹\n",
      "\n",
      "[Temperature = 1.5 (ì°½ì˜ì )]\n",
      "ëª¨ì ì“´ ì‘ì€ íŠ¸ë¦¬ë¼ë¦¬ì˜ ë²—êµë¶€í™œì „  \n",
      "\n",
      "ë¯¸ì§€ê·¼í•œ ì•„ì¹¨ í–‡ì‚´ ì•„ë˜ì—ì„œ ìƒë¹„ëŸ‰ì´ ê²°ì§“í•˜ëŠ” ì‘ì€ ì‚¬ë§‰ì´ ë‚¡\n"
     ]
    }
   ],
   "source": [
    "def chat_with_params(messages, temperature=0.7, top_p=0.9, \n",
    "                     max_tokens=200, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    ìƒì„¸í•œ íŒŒë¼ë¯¸í„°ë¡œ ì±„íŒ…ì„ ì§„í–‰í•œë‹¤\n",
    "    \n",
    "    Args:\n",
    "        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "        temperature: ëœë¤ì„± ì¡°ì ˆ (0.0 ~ 2.0)\n",
    "        top_p: ëˆ„ì  í™•ë¥  ì„ê³„ê°’ (0.0 ~ 1.0)\n",
    "        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
    "        presence_penalty: ì£¼ì œ ë‹¤ì–‘ì„± ì¡°ì ˆ (-2.0 ~ 2.0)\n",
    "    \n",
    "    Returns:\n",
    "        ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ì§§ì€ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\"}\n",
    "]\n",
    "\n",
    "print(\"=== íŒŒë¼ë¯¸í„° ë¹„êµ ì˜ˆì œ ===\")\n",
    "\n",
    "# ë‚®ì€ temperature (ë” ê²°ì •ì )\n",
    "print(\"\\n[Temperature = 0.2 (ê²°ì •ì )]\")\n",
    "response_low = chat_with_params(test_messages, temperature=0.2)\n",
    "print(response_low)\n",
    "\n",
    "# ë†’ì€ temperature (ë” ì°½ì˜ì )\n",
    "print(\"\\n[Temperature = 1.5 (ì°½ì˜ì )]\")\n",
    "response_high = chat_with_params(test_messages, temperature=1.5)\n",
    "print(response_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í† í° ì‚¬ìš©ëŸ‰ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„ ===\n",
      "\n",
      "ì‘ë‹µ: í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì€ ì¸í„°ë„·ì„ í†µí•´ ì„œë²„, ìŠ¤í† ë¦¬ì§€, ë°ì´í„°ë² ì´ìŠ¤, ë„¤íŠ¸ì›Œí‚¹ ë“± ì»´í“¨í„° ìì›ì„ â€œì„œë¹„ìŠ¤â€ í˜•íƒœë¡œ ì œê³µí•˜ê³ , ì‚¬ìš©ìê°€ í•„ìš”í•  ë•Œë§Œ ì‚¬ìš©í•˜ê³  ë¹„ìš©ì„ ì§€ë¶ˆí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "í† í° ì‚¬ìš©ëŸ‰:\n",
      "  - í”„ë¡¬í”„íŠ¸ í† í°: 97\n",
      "  - ìƒì„± í† í°: 125\n",
      "  - ì´ í† í°: 222\n"
     ]
    }
   ],
   "source": [
    "def chat_with_usage(messages):\n",
    "    \"\"\"\n",
    "    í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì±„íŒ…í•œë‹¤\n",
    "    \n",
    "    Args:\n",
    "        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "        ì‘ë‹µ ë‚´ìš©ê³¼ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "        \"completion_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# í† í° ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ë‹¹ì‹ ì€ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” AIì…ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "result = chat_with_usage(messages)\n",
    "\n",
    "print(\"=== í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„ ===\")\n",
    "print(f\"\\nì‘ë‹µ: {result['content']}\")\n",
    "print(f\"\\ní† í° ì‚¬ìš©ëŸ‰:\")\n",
    "print(f\"  - í”„ë¡¬í”„íŠ¸ í† í°: {result['prompt_tokens']}\")\n",
    "print(f\"  - ìƒì„± í† í°: {result['completion_tokens']}\")\n",
    "print(f\"  - ì´ í† í°: {result['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ì‹¤ìš©ì ì¸ ì±„íŒ… í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== vLLM ì±„íŒ…ë´‡ ì˜ˆì œ ===\n",
      "\n",
      "ì‚¬ìš©ì: íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ”?\n",
      "ë´‡: ## ë¦¬ìŠ¤íŠ¸(`list`) vs íŠœí”Œ(`tuple`) â€“ í•µì‹¬ ì°¨ì´ì \n",
      "\n",
      "| êµ¬ë¶„ | ë¦¬ìŠ¤íŠ¸ | íŠœí”Œ |\n",
      "|------|--------|------|\n",
      "| **ë¬¸ë²•** | `my_list = [1, 2, 3]` | `my_tuple = (1, 2, 3)`<br>ë‹¨ì¼ ìš”ì†ŒëŠ” `a = (1,)`ì²˜ëŸ¼ ì‰¼í‘œê°€ í•„ìš” |\n",
      "| **ê°€ë³€ì„±** | **ê°€ë³€(mutable)** â€“ ìƒì„± í›„ì—ë„ ìš”ì†Œë¥¼ ì¶”ê°€Â·ì‚­ì œÂ·ëŒ€ì²´ ê°€ëŠ¥ | **ë¶ˆë³€(immutable)** â€“ í•œ ë²ˆ ë§Œë“¤ë©´ ë‚´ìš©ì´ ë°”ë€Œì§€ ì•ŠìŒ |\n",
      "| **í™œìš©ë„** | ë™ì ìœ¼ë¡œ í¬ê¸°ê°€ ë°”ë€ŒëŠ” ì»¬ë ‰ì…˜, ë°ì´í„°ë¥¼ ì¶”ê°€/ì‚­ì œí•  ë•Œ | ê³ ì •ëœ ë°ì´í„° ì§‘í•©, í•¨ìˆ˜ì—ì„œ ì—¬ëŸ¬ ê°’ì„ ë°˜í™˜, dict key ë“±ìœ¼ë¡œ í™œìš© |\n",
      "| **ë©”ëª¨ë¦¬/ì†ë„** | ì•½ê°„ ë” í° ë©”ëª¨ë¦¬ ì‚¬ìš©, ìš”ì†Œ ì ‘ê·¼ì€ íŠœí”Œë³´ë‹¤ ëŠë¦¼ | ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ì ê³ , ì¸ë±ì‹±Â·ë°˜ë³µì´ ë¹ ë¦„ |\n",
      "| **ë©”ì„œë“œ** | `append`, `extend`, `insert`, `remove`, `pop`, `sort`, `reverse` ë“± | ë©”ì„œë“œê°€ ê±°ì˜ ì—†ìŒ (ì¸ë±ì‹±Â·ìŠ¬ë¼ì´ì‹±ë§Œ ê°€ëŠ¥) |\n",
      "| **ì•ˆì „ì„±** | ê°’ì´ ë°”ë€”\n",
      "\n",
      "ì‚¬ìš©ì: ì–´ëŠ ê²ƒì„ ì–¸ì œ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "ë´‡: None\n",
      "\n",
      "ëŒ€í™” í†µê³„:\n",
      "  - ì´ ë©”ì‹œì§€ ìˆ˜: 5\n",
      "  - ëˆ„ì  í† í° ì‚¬ìš©: 1523\n"
     ]
    }
   ],
   "source": [
    "class VLLMChatbot:\n",
    "    \"\"\"\n",
    "    vLLMì„ ì‚¬ìš©í•˜ëŠ” ì±„íŒ…ë´‡ í´ë˜ìŠ¤ë‹¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_message=None, model=MODEL_NAME):\n",
    "        \"\"\"\n",
    "        ì±„íŒ…ë´‡ì„ ì´ˆê¸°í™”í•œë‹¤\n",
    "        \n",
    "        Args:\n",
    "            system_message: ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„ íƒì‚¬í•­)\n",
    "            model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.history = []\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •\n",
    "        if system_message:\n",
    "            self.history.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            })\n",
    "    \n",
    "    def chat(self, user_message, temperature=0.7, max_tokens=500):\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë°›ì•„ ì‘ë‹µì„ ìƒì„±í•œë‹¤\n",
    "        \n",
    "        Args:\n",
    "            user_message: ì‚¬ìš©ì ë©”ì‹œì§€\n",
    "            temperature: ì°½ì˜ì„± ì¡°ì ˆ\n",
    "            max_tokens: ìµœëŒ€ ìƒì„± í† í°\n",
    "        \n",
    "        Returns:\n",
    "            ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ\n",
    "        \"\"\"\n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
    "        self.history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        # ì‘ë‹µ ìƒì„±\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.history,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        # ì‘ë‹µ ì¶”ì¶œ\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€\n",
    "        self.history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "        \n",
    "        # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì \n",
    "        self.total_tokens += response.usage.total_tokens\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def get_history_length(self):\n",
    "        \"\"\"\n",
    "        í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´ë¥¼ ë°˜í™˜í•œë‹¤\n",
    "        \"\"\"\n",
    "        return len(self.history)\n",
    "    \n",
    "    def get_total_tokens(self):\n",
    "        \"\"\"\n",
    "        ëˆ„ì  í† í° ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•œë‹¤\n",
    "        \"\"\"\n",
    "        return self.total_tokens\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"\n",
    "        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•œë‹¤\n",
    "        \"\"\"\n",
    "        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìœ ì§€\n",
    "        system_messages = [msg for msg in self.history if msg[\"role\"] == \"system\"]\n",
    "        self.history = system_messages\n",
    "        self.total_tokens = 0\n",
    "\n",
    "# ì±„íŒ…ë´‡ ì‚¬ìš© ì˜ˆì œ\n",
    "print(\"=== vLLM ì±„íŒ…ë´‡ ì˜ˆì œ ===\")\n",
    "\n",
    "# ì±„íŒ…ë´‡ ìƒì„±\n",
    "bot = VLLMChatbot(\n",
    "    system_message=\"ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    ")\n",
    "\n",
    "# ëŒ€í™” 1\n",
    "response_1 = bot.chat(\"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ”?\")\n",
    "print(f\"\\nì‚¬ìš©ì: íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ”?\")\n",
    "print(f\"ë´‡: {response_1}\")\n",
    "\n",
    "# ëŒ€í™” 2\n",
    "response_2 = bot.chat(\"ì–´ëŠ ê²ƒì„ ì–¸ì œ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?\")\n",
    "print(f\"\\nì‚¬ìš©ì: ì–´ëŠ ê²ƒì„ ì–¸ì œ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?\")\n",
    "print(f\"ë´‡: {response_2}\")\n",
    "\n",
    "# í†µê³„ ì¶œë ¥\n",
    "print(f\"\\nëŒ€í™” í†µê³„:\")\n",
    "print(f\"  - ì´ ë©”ì‹œì§€ ìˆ˜: {bot.get_history_length()}\")\n",
    "print(f\"  - ëˆ„ì  í† í° ì‚¬ìš©: {bot.get_total_tokens()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
